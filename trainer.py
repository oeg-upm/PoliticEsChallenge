# -*- coding: utf-8 -*-
"""TFG Julio Pino - Version Pablo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pIQJIRVX7kqTQDg_H42U-10RDcK03fBb

# SentimentTweets

## Dependencies
"""

#!pip install transformers -q. #transformers==4.18.0
#!pip install datasets -q. #datasets==2.1.0
#!python -m spacy download es_core_news_sm
#!python -m spacy link es_core_news_sm es



"""## Imports"""

import pandas as pd
from datasets import Dataset, load_metric
from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer
import numpy as np
import pyarrow as pa
import pyarrow.dataset as ds
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
import re
import spacy

"""## Global variables"""

#MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-cased'
MODEL_NAME = 'PlanTL-GOB-ES/roberta-base-bne'

"""### Tokenizer"""

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

"""### Model"""

columns = [ 'gender', 'profession','ideology_binary', 'ideology_multiclass'] #
models = {}
for key in columns:
    if key == 'ideology_multiclass':
        models[key] = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4)
    else:
        models[key] = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)



"""## Functions"""

max_length = max_length=models['gender'].config.max_position_embeddings
max_length = max_length if max_length < 400 else 400

#nlp = spacy.load("es")
#def lemmatizer(text):  
  #doc = nlp(text)
  #return ' '.join([word.lemma_ for word in doc])

import re
import nltk

#def remove_stopwords(text, lang):

def process_long_url(url):
  url = re.sub(r'https://[:a-zA-Z_0-9-.&?]*', ' ', url) 
  url = re.sub(r'http://[:a-zA-Z_0-9-.&?]*', ' ', url) 
  url = url.replace('www.','')
  url = url.replace('.com','')
  url = url.replace('/',' ')
  url = url.replace('-',' ')
  return url

def process_urls(text):
  res = re.findall(r'http[:/a-zA-Z_0-9-.&?]*', text)
  for i in res:
    if len(i)<30: # corta
      
      text= text.replace(i,' ')
    else:
      text = text.replace(i,process_long_url(i))
  return text

def remove_users(text):
  return re.sub(r'@[a-zA-Z_0-9-]*', ' ', text)


def remove_numbers(text):
  return re.sub(r'[0123456789]+', ' ', text)


def transform_hastag(hashtag):
  newHastag=''
  for c in hashtag:
    if c.isupper() == True:
      newHastag=newHastag+' '+c
    else:
      newHastag=newHastag+c

  newHastag =newHastag.strip()
  newHastag=newHastag.replace('#',' ')
  return newHastag

def process_hashtags(text):
  res = re.findall(r'#[a-zA-Z_0-9]*', text)
  for i in res:
    text = text.replace(i, transform_hastag(i))
  return text


def process_tweet(text, lang = None):
  
  text = remove_users(text)
  text = process_urls(text)

  text = process_hashtags(text)
  text = text.replace('/',' ')
  text = remove_numbers(text)
  #Remove multiple whitespaces (I think that in the tokenization stage this becomes irrelevant but I've added just in case).
  text = ' '.join(text.split())
  #Option to remove stopwords, I've removed from the stopword list no and not for cases like 'no deberÃ­as salir de la cocina' (although, it keeps soundig misogynistic)
  #text = remove_stopwords(text, lang)

  
  return text


def text_preprocessing(group):
    """
    - Remove entity mentions (eg. '@united')
    - Correct errors (eg. '&amp;' to '&')
    @param    text (str): a string to be processed.
    @return   text (Str): the processed string.
    """
    result = []

    for text in group:
        text = process_tweet(text)
        # Remove '@name'
        #text = re.sub(r'(@.*?)[\s]', ' ', text)

        # Replace '&amp;' with '&'
        #text = re.sub(r'&amp;', '&', text)

        # Remove trailing whitespace
        #text = re.sub(r'\s+', ' ', text).strip()

        # Remove punctuation
        #text = re.sub(r'[^\w\s]', '', text)

        result.append(text)

    return result

def preprocessing_tokenizer(input):
  #return tokenizer(input["text"], truncation=True, padding=True, max_length=max_length)
  return tokenizer(text_preprocessing(input["text"]), truncation=True, padding=True, max_length=512)

def preprocess(*args):
    result = []
    for arg in args:
        aux = arg.map(preprocessing_tokenizer, batched=True,load_from_cache_file=False)
        aux.set_format(type='torch', columns=['input_ids', 'label', 'attention_mask'])
        result.append(aux)

    return result

def compute_metrics(eval_preds):
    logits, y_true = eval_preds
    y_pred = np.argmax(logits, axis=-1)
    return { 'precision': precision_score(y_true, y_pred, average='macro'), 'recall': recall_score(y_true, y_pred, average='macro'), 'accuracy': accuracy_score(y_true, y_pred), 'f1': f1_score(y_true, y_pred, average='macro') }

def createDatasets(*args):
    result = []
    for arg in args:
        result.append(Dataset.from_pandas(arg))
    
    return result

"""## Datasets"""

df_train = pd.read_csv('politics_train_evaluation.tsv', sep='\t')
df_test = pd.read_csv('politics_test_evaluation.tsv', sep='\t')
df_train.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], inplace=True)
df_test.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], inplace=True)
df_train, df_eval = train_test_split(df_train, test_size=0.15, random_state=2022)
ds_group = {}
for key in columns:
    new_df_train = df_train[[key, 'text']]
    new_df_eval = df_eval[[key, 'text']]
    new_df_test = df_test[[key, 'text']]
    new_df_train.rename(columns={key: 'label'}, inplace=True)
    new_df_eval.rename(columns={key: 'label'}, inplace=True)
    new_df_test.rename(columns={key: 'label'}, inplace=True)
    ds_group[key] = createDatasets(new_df_train, new_df_eval, new_df_test)

"""## Preprocessing"""

preprocessed_group = {}
for key in columns:
    preprocessed_group[key] = preprocess(ds_group[key][0], ds_group[key][1], ds_group[key][2])

"""## Training arguments"""

trainers = {}
for key in columns:
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    training_args = TrainingArguments(
        output_dir="./results",
        learning_rate=1e-5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=4,
        weight_decay=0.01,
        evaluation_strategy="epoch",
    )

    trainer = Trainer(
        model=models[key],
        args=training_args,
        train_dataset=preprocessed_group[key][0],
        eval_dataset=preprocessed_group[key][1],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    trainers[key] = trainer



"""## Training"""

for key in columns:
    trainers[key].train()
    models[key].save_pretrained('./model_' + key)
    tokenizer.save_pretrained('./model_' + key)



#trainers['ideology_binary'].train()

"""## Evaluating"""

f1_group = {}
for key in columns:
    result = trainers[key].evaluate(preprocessed_group[key][2])
    f1_group[key] = result['eval_f1']

final_f1 = 0
for key in columns:
    final_f1 += f1_group[key]
    print('f1_score for ' + key + ': ' + str(f1_group[key]))

print('final f1 score: ' + str(final_f1/len(f1_group)))

"""## Testing"""

predictions = {}
predictions['user'] = df_test['user'].values
for key in columns:
    output = trainers[key].predict(preprocessed_group[key][2])
    print("\n*** OUTPUT ***\n" + str(output[0]) + "\n")
    predictions[key] = np.argmax(output[0], axis=-1)

df_output = pd.DataFrame(predictions)



df_output.to_csv("./submision_file.tsv", sep="\t", index=False)

#for key in columns:
    

#!zip -r "./drive/MyDrive/TFG/Sentiment Politics/politics_model_00.zip" "./drive/MyDrive/TFG/Sentiment Politics/politics_model/"